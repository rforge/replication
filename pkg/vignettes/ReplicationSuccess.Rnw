% \VignetteEngine{knitr::knitr}
% \VignetteEncoding{UTF-8}
% \VignetteIndexEntry{Introduction to ReplicationSuccess}
% \VignetteDepends{knitr}

\documentclass[a4paper, 11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphics}
\usepackage{amsmath, amssymb}
\usepackage[round]{natbib}
\bibliographystyle{plainnat}
\usepackage{todonotes}
\input{newCommands.tex} 

% margins %
\usepackage[a4paper, total={6.5in, 10in}]{geometry}

% title, author, date, etc.
\title{\textbf{Design and Analysis of Replication Studies with \texttt{ReplicationSuccess}}}
\author{\textbf{Leonhard Held, Charlotte Micheloud, Samuel Pawel} \\
Epidemiology, Biostatistics and Prevention Institute (EBPI) \\
Center for Reproducible Science (CRS) \\
University of Zurich, Switzerland}
\date{}%\today}

% hyperref options
\usepackage{hyperref}  
\hypersetup{
  bookmarksopen=true, 
  breaklinks=true,
  pdftitle={ReplicationSuccess}, 
  pdfauthor={Leonhard Held, Charlotte Micheloud, Samuel Pawel},
  colorlinks=true,
  linkcolor=red,
  anchorcolor=black,
  citecolor=blue,
  urlcolor=magenta,
}

\begin{document}
<< "knitr-options", echo = FALSE >>=
library(knitr)
opts_chunk$set(size = "small",
               fig.height = 4,
               fig.align = "center",
               cache = FALSE,
               message = FALSE,
               warning = FALSE)
@

\maketitle


\begin{center}
\begin{minipage}{0.65\textwidth}
  \rule{\textwidth}{0.4pt}
  {\small
  \centering \textbf{Abstract} \\
  This vignette provides an introduction to the \textsf{R} package
  \texttt{ReplicationSuccess}. The package contains utilities for planning and
  analysing replication studies. Traditional methods based on statistical 
  significance and confidence intervals, as well as more recently developed methods 
  such as the sceptical $p$-value \citep{Held2020} are included.
  The functionality of the package is illustrated using data sets from four 
  large-scale replication projects which come also with the package.
  }
  \rule{\textwidth}{0.4pt}
\end{minipage}
\end{center}

\section{Introduction}
Over the course of the last decade, the conduct of replication studies
has increased substantially. These developments were mainly caused by
the so-called ``replication crisis'' in the social and life-sciences.
However, there is no consensus on which statistical analysis approach
should be used to assess whether a replication study successfully 
replicated an original discovery. Moreover, depending on the chosen
analysis approach, the statistical considerations in the design
of the replication study differ. 

The \textsf{R} package \texttt{ReplicationSuccess} provides 
functionality to analyse and plan replication studies in several different ways.
Specifically, functions for power and samples size calculations based 
on statistical significance, as well as based on more recent methods, such as 
the sceptical $p$-value \citep{Held2020}, are included.
In this vignette the usage of the package is illustrated on the data sets
from four large-scale replication projects which are also included in the 
package.

\subsection{Statistical framework}
\texttt{ReplicationSuccess} assumes a simple but general and practically relevant 
statistical framework for effect sizes. Specifically, after a suitable transformation the effect estimates are 
assumed to be approximately normally distributed with known variances
which do not depend on the effect anymore. 
The same framework is also common in the meta-analysis literature and
can for example be applied to mean differences, odds ratios 
(log transformation), or correlation coefficients (Fisher $z$-transformation). 


Moreover, most functions in \texttt{ReplicationSuccess} take unitless quantities as inputs. In particular, the $z$-values 
$z_o = \hat{\theta}_o/\sigma_o$, $z_r = \hat{\theta}_r/\sigma_r$, 
and the variance ratio $c = \sigma^2_o/\sigma^2_r$ 
($\hat{\theta}$ denotes an effect estimate
and $\sigma^2$ the corresponding variance, the subscripts indicate original
or replication). 
Assuming that the standard errors of the effect estimates only depend on some unit variance $\kappa^2$ and inversely on the sample size of the study, 
\ie $\sigma^2_o = \kappa^2/n_o$ and $\sigma^2_r = \kappa^2/n_r$, 
the variance ratio is also the relative sample size 
$c = \sigma^2_o/\sigma^2_r = n_r/n_o$. 
For this reason, all functions from \texttt{ReplicationSuccess} 
used for sample size computations return $c$. 


\section{Data sets}
\texttt{ReplicationSuccess} includes data from four replication projects, all with 
a ``one-to-one'' design (\ie one replication for one original study).
They come from the following projects:

\begin{itemize}
\item \textbf{Reproducibility Project: Psychology:}
In the \emph{Reproducibility Project: Psychology} 100 replications of studies from the field of psychology were conducted \citep{Opensc2015}.
The original studies were published in three major Psychology journals in the year 2008.
Only the study pairs of the ``meta-analytic subset'' are included here, which consists of 73 studies where the standard error of the Fisher $z$-transformed effect estimates can be computed \citep{Johnson2016}.

\item \textbf{Experimental Economics Replication Project:}
This project attempted to replicate 18 experimental economics studies published between 2011 and 2015 in two high impact economics journals \citep{Camerer2016}.
For this project a \emph{prediction market} was also conducted in order to estimate the peer beliefs about whether a replication will result in a statistically significant result.
Prediction markets are a tool to aggregate beliefs of market participants regarding the possibility of an investigated outcome and they have been used successfully in numerous domains, \eg sports and politics \citep{Dreber2015}.
The estimated peer beliefs are also included for each study pair.

\item \textbf{Social Sciences Replication Project:}
This project involved 21 replications of studies on the social sciences published in the journals \emph{Nature} and \emph{Science} between 2010 and 2015 \citep{Camerer2018}.
As in the experimental economics replication project, a prediction market to estimate peer beliefs about the replicability of the original studies was conducted and the resulting belief estimates are also provided in the package. In this project, the replications were conducted in two stages. In stage 1, the replication studies had 90\% power to detect 75\% of the original effect estimate. Data collection eas stopped if a two-sided $p$-value $< 0.05$ and an effect in the same direction as the original were found. If not, data collection was continued in stage 2 to have 90\% power to detect 50\% of the original effect size for the first and second data collection pooled. 

\item \textbf{Experimental Philosophy Replicability Project:}
In this project, 40 replications of experimental philosophy studies were carried out.
The original studies had to be  published between 2003 and 2015 in one of 35 journals in which experimental philosophy research is usually published (a list defined by the coordinators of this project) and they had to be listed on the experimental philosophy page of the Yale university \citep{Cova2018}.
The data from the subset of 31 study pairs where effect estimates on correlation scale
as well as effective sample size for both the original and replication were available
are included in the package.
\end{itemize}

In all data sets, effect estimates are provided as correlation coefficients 
($r$), as well as Fisher $z$-transformed correlation coefficients
($\hat{\theta} = \text{tanh}^{-1}(r)$). 
In the descriptive analysis of data from replication projects it has become common
practice to transform effect sizes to the correlation scale, because correlations 
are bounded to the interval between minus one and one and 
thus easy to compare and interpret. Design and statistical analysis, on the other hand,
is then usually carried out on a scale where the estimates are approximately normally 
distributed. For correlation coefficients this is the case after applying the Fisher $z$-transformation, which leads to their variance asymptotically being only a function 
of the study sample size $n$, \ie $\Var(\hat{\theta}) = 1/(n - 3)$ \citep{Fisher1921}.


The data can be loaded with the command \texttt{data("RProjects")}. For a
description of the variables see the documentation with \texttt{?RProjects}.
An extended version of the Social Sciences Replication Project including the details of stages one and two can be loaded with \texttt{data("SSRP")}.
It is a good idea to first compute the unitless quantities $z_o$, $z_r$ and
$c$, since most functions of the package use them as input. 
We also use the function \texttt{z2p} to compute the one-sided 
$p$-values for original and replication study. As all original estimates are
positive, we specify the argument \texttt{alternative} to \texttt{"greater"}.

<< "data-loading" >>=
library(ReplicationSuccess)
data("RProjects")
str(RProjects)

## computing zo, zr, c
RProjects$zo <- with(RProjects, fiso/se_fiso)
RProjects$zr <- with(RProjects, fisr/se_fisr)
RProjects$c <- with(RProjects, se_fiso^2/se_fisr^2)

## computing one-sided p-values for alternative = "greater"
RProjects$po1 <- z2p(z = RProjects$zo, alternative = "greater")
RProjects$pr1 <- z2p(z = RProjects$zr, alternative = "greater")
@

Note that each variable ending with an \texttt{o} is associated with the original,
while each variable ending with an \texttt{r} is associated with the replication.
Plotting the original versus the replication effect estimate on the correlation scale
gives a good overview of the data.

<< "plot-projects", fig.height = 5 >>=
## plots of effect estimates
par(mfrow = c(2, 2), las = 1, mai = rep(0.65, 4))
for (p in unique(RProjects$project)) {
    data_project <- subset(RProjects, project == p)
    significant <- ifelse(data_project$pr < 0.05, "darkred", "black")
    plot(rr ~ ro, data = data_project, ylim = c(-0.5, 1), col = significant,
         xlim = c(-0.1, 1), main = p, xlab = expression(italic(r)[o]), 
         cex = 0.7, pch = 19, ylab = expression(italic(r)[r]))
    legend("topleft", legend = "significant", pch = 20, col = "darkred", bty = "n")
    abline(h = 0, lty = 2)
    abline(a = 0, b = 1, col = "grey")
}
@

In most cases the replication estimate is smaller than the corresponding 
original estimate. Furthermore, a substantial number of the replication estimates
do not achieve statistical significance at one-sided 2.5\% level, while almost all original
estimates did. 

\section{Design and analysis of replication studies}
Although a replication study needs to be planned and conducted before the results
can be analysed, we will first discuss the particular analysis approaches. 
We do this because the chosen analysis strategy substantially 
influences the design of a replication study. 
In the design phase of a replication study, we focus only on the 
sample size determination.

\subsection{Statistical significance}
\paragraph{Analysis}
One of the most commonly used approaches to analyse the result of a replication study
is to declare a replication study successful if the replication estimate achieves 
the same statistical significance status as the original estimate and also goes
in the same direction. 
There are some variations of this approach, for example,
\citet{Camerer2016} only assessed whether the replication effect is significant in
the same direction, but not whether the original effect shows the same significance 
status. 


For the four data sets, we can simply check whether the (two-sided) $p$-values of original and
replication are both below the conventional threshold 0.05 and whether the directions
of the effects are the same.

<< >>=
for (p in unique(RProjects$project)) {
    data_project <- subset(RProjects, project == p)
    significant_O <- data_project$po < 0.05
    significant_R <- data_project$pr < 0.05
    success <- (significant_O == TRUE) & (significant_R == TRUE) & 
        (sign(data_project$fiso) == sign(data_project$fisr))
    cat(paste0(p, ": \n"))
    cat(paste0(round(mean(significant_O)*100, 1), "% original studies significant (", 
               sum(significant_O), "/", length(significant_O), ")\n"))
    cat(paste0(round(mean(significant_R)*100, 1), "% replications significant (", 
               sum(significant_R), "/", length(significant_R), ")\n"))
    cat(paste0(round(mean(success)*100, 1), 
               "% both studies significant in the same direction (", 
               sum(success), "/", length(success), ")\n \n"))
}
@

Despite its appealing simplicity, assessing replication success with statistical
significance is often criticized. 
For example, non-significant replication results are expected if the original finding
was a false positive (\eg with 95\% probability if the two-sided significance level is 5\%), 
on the other hand they are also expected with non-negligible probability if the
underlying effect is present \citep{Goodman1992}.
Conversely, when the effect estimate of the replication is much smaller than the
estimate from the original study, statistical significance can still be achieved 
by simply increasing the sample size.

\paragraph{Design}
Selecting the same sample size in the replication study as in the original study may
lead to a severely underpowered design and as a result, true effects may not be 
detected. 
To assure that the replication study reliably detects true effects, 
the studies should be well-powered. In classical sample size planning,
usually a clinically relevant effect is specified and the sample size is then 
determined so that it can be detected with a certain power. Luckily, in the
replication setting the clinically relevant effect does not need to be specified but
can be replaced with the effect estimate from the original study. 
However, using the standard sample size calculation approach is not well suited,
because the uncertainty of the original effect estimate is ignored.

One way of tackling this issue is to use a Bayesian approach, incorporating
the original estimate and its precision into a design prior that is used for power
calculations. This corresponds to the concept of ``predictive power'' and generally 
leads to larger sample sizes than the standard method. 
In practice, however, often more ad hoc approaches are used. For instance, the
original estimate is just shrunken by an (arbitrary) constant, \eg it was halved in the 
sociel sciences replication project, and standard sample size calculations are
then carried out. 


Using the function \texttt{sampleSizeSignificance},
it is straightforward to plan the sample size of the replication study with the
just mentioned approaches. The argument \texttt{designPrior} allows to carry out 
sample size planning based on classical power ignoring the uncertainty 
(\texttt{"conditional"}) or based on predictive power (\texttt{"predictive"}). 
Moreover, ad hoc shrinkage can be specified with the argument \texttt{shrinkage}.
It must be noted that the function \texttt{sampleSizeSignificance}, as well as most of the
functions from the package, takes $z$-values and no $p$-values as arguments. 
The conversion between the two measures can easily be done using the function \texttt{p2z}.

The following code shows a few examples. Note that the function returns the required
relative sample size $c = n_r/n_o$, \ie by which factor the sample size of the replication 
needs to be changed compared to the original study.
<< >>=
sampleSizeSignificance(zo = 2.5, power = 0.8, level = 0.05, designPrior = "conditional")
sampleSizeSignificance(zo = 2.5, power = 0.8, level = 0.05, designPrior = "predictive")
sampleSizeSignificance(zo = 2.5, power = 0.8, level = 0.05, designPrior = "conditional",
                       shrinkage = 0.25)
@

Figure \ref{fig:powerSignificance} shows the power to achieve significance in the
replication as a function of either the (two-sided) $p$-value or the $z$-value
of the original study. If the original estimate was just significant at the 0.05 level,
the probability for significance in the replication is just about 0.5 for conditional
and predictive power.
This result was first mentioned by \citet{Goodman1992} already two decades ago, 
yet many practitioners of statistics still find it counterintuitive, because they 
confuse type I error rates with replication probabilities. Thus, for the replication
to achieve significance with high probability, the sample size needs to be
increased compared to the original if the the evidence for the original discovery 
was only weak or moderate (Figure \ref{fig:sampleSizeSignificance}).



\begin{figure}[!h]
<< "plot-powerSignificance", echo = FALSE, fig.height = 4 >>=
po <- seq(0.0001, 0.05, 0.0001)

## plot power
plot(po, powerSignificance(zo = p2z(po), designPrior = "conditional")*100,
     type = "l", ylim = c(0, 100), lwd = 1.5, ylab = "Power (%)", 
     xlab = expression(italic(p)[o]), las = 1)
axis(side = 3, at = seq(0.0, 0.05, by = 0.01), 
     labels = c("", round(p2z(p = seq(0.01, 0.05, by = 0.01)), 2)))
mtext(text = expression(paste("|", italic(z)[o], "|")), side = 3, line = 2)
abline(h = 50, col = "#333333B3", lty = 3)
lines(po, powerSignificance(zo = p2z(po), designPrior = "predictive")*100, lwd = 2, lty = 2)
legend("topright", legend = c("conditional", "predictive"), 
       title = "Design prior", lty = c(1, 2), lwd = 1.5, bty = "n")
@
\caption{Power to achieve significance at the one-sided 2.5\% level in replication as a function of (two-sided) $p$-value or $z$-value of original study using the same sample size
as in the original study.}
\label{fig:powerSignificance}
\end{figure}


\begin{figure}[!h]
<< "plot-sampleSizeSignificance", echo = FALSE, fig.height = 4 >>=
## plot sample size 
plot(po, sampleSizeSignificance(zo = p2z(po), designPrior = "conditional", power = 0.8),
     type = "l", ylim = c(0.5, 10), log = "y", lwd = 1.5, 
     ylab = expression(paste("Relative sample size ", n[r]/n[o])),
     xlab = expression(italic(p)[o]), las = 1)
axis(side = 3, at = seq(0.0, 0.05, by = 0.01), 
     labels = c("", round(p2z(p = seq(0.01, 0.05, by = 0.01)), 2)))
mtext(text = expression(paste("|", italic(z)[o], "|")), side = 3, line = 2)
abline(h = 1, col = "#333333B3", lty = 3)
lines(po, sampleSizeSignificance(zo = p2z(po), designPrior = "predictive", power = 0.8), 
      lwd = 2, lty = 2)
legend("topleft", legend = c("conditional", "predictive"), 
       title = "Design prior", lty = c(1, 2), lwd = 1.5, bty = "n")
@
\caption{Relative sample size to achieve significance at the one-sided 2.5\% level with 80\% power as
a function of (two-sided) $p$-value or $z$-value of original study.}
\label{fig:sampleSizeSignificance}
\end{figure}




\subsection{Compatibility of effect size}
\paragraph{Analysis}
Another analysis approach that has been used is to compare the effect estimates 
from original
and replication study. A reasonable way to assess whether the observed estimates are
compatible is to check whether the replication
estimate is contained within its prediction interval based on the original
estimate \citep{Patil2016}. With the function \texttt{predictionInterval}, a prediction
interval of the replication effect estimate can be computed under different predictive
distributions which depend on the design prior. The default design prior
\texttt{"predictive"} is likely the choice most people would want to use as it takes
into account the uncertainty of the original estimate without shrinking it.

For the four data sets, we can easily compute the prediction intervals and then check 
whether the replication estimates are contained within them. For easier visual 
assessment we transform the intervals and estimates back to the correlation scale.

<< "plot-predictionInterval", fig.height = 6 >>=
## compute prediction intervals for replication projects
par(mfrow = c(2, 2), las = 1, mai = rep(0.65, 4))
for (p in unique(RProjects$project)) {
    data_project <- subset(RProjects, project == p)
    PI <- predictionInterval(thetao = data_project$fiso, 
                             seo = data_project$se_fiso, 
                             ser = data_project$se_fisr)
    ## transforming back to correlation scale 
    PI <- tanh(PI)
    within <- (data_project$rr < PI$upper) & (data_project$rr > PI$lower)
    coverage <- mean(within)
    color <- ifelse(within == TRUE, "#333333B3", "#8B0000B3")
    study <- seq(1, nrow(data_project))
    plot(data_project$rr, study, col = color, pch = 20, 
         xlim = c(-0.5, 1), xlab = expression(italic(r)[r]), ylab = "Study",
         main = paste0(p, ": ", round(coverage*100, 0), "% coverage"))
    arrows(PI$lower, study, PI$upper, study, length = 0.02, angle = 90, code = 3, col = color)
    abline(v = 0, lty = 3)
}
@

The criticism that this approach receives is that for studies which are underpowered,
the prediction intervals will become very wide.
This in turn can lead to very different effect estimates being compatible, 
\eg even ones that go in the opposite direction, ultimately providing 
no information about the effect itself (which actually happens in some 
cases in the economics and philosophy data sets).


\subsection{The sceptical $p\,$-value}
\paragraph{Analysis}
The \emph{sceptical $p$-value}, a new quantitative measure of replication success was recently proposed by \citet{Held2020}. The \emph{sceptical $p$-value} arises from 
combining the intrinsic credibility method \citep{Matthews2001a} with the
prior-predictive check \citep{Box1980}. Specifically, using Bayes theorem in reverse, 
the prior distribution of the effect 
size can be determined such that conditional on the original study, the $(1 - \alpha)$
credible interval of the posterior distribution of the effect just includes zero.
This prior corresponds to the objection of a sceptic who argues
that the original finding is no longer significant if combined with a sufficiently
sceptical prior. 
Replication success at level $\alpha$ is then achieved if the tail probability of 
the replication estimate under its prior predictive distribution 
is smaller than $\alpha$, rendering the objection of the sceptic unrealistic.

\begin{figure}[!h]
<< "plot-pSceptical", echo = FALSE, fig.height = 4 >>=
## Functions
ssv <- function(zo, so, a) {
    tau2 <- so^2/(zo^2/qnorm(p = a/2, lower.tail = FALSE)^2 - 1)
    return(tau2)
}

## Parameters and computations
options(scipen = 5)
theta_o <- 0.57
theta_r <- 0.33
so <- 0.165
sr <- 0.165
c <- so^2/sr^2
zo <- theta_o/so
zr <- theta_r/sr
alpha <- 0.05
za <- qnorm(p = alpha/2, lower.tail = FALSE)
ps <- signif(pSceptical(zo = zo, zr = zr, c = c, 
                        alternative = "two.sided"), 2)
ps_tilde <- signif(pSceptical(zo = zo, zr = zr, c = c, 
                              alternative = "one.sided"), 2)
tau <- sqrt(ssv(zo, so, alpha))
s2_p <- 1/(1/so^2 + 1/tau^2)
mu_p <- s2_p*theta_o/so^2



## Plot
df <- data.frame(estimate = factor(c("Original Study", 
                                     "Posterior", 
                                     "Sceptical Prior", 
                                     "Replication Study"),
                                   levels = c("Original Study", 
                                              "Posterior", 
                                              "Sceptical Prior", 
                                              "Replication Study")),
                 theta = c(theta_o, 
                           mu_p, 
                           0, 
                           theta_r),
                 lower = c(theta_o - za*so,
                           mu_p - za*sqrt(s2_p), 
                           0 - za*tau, 
                           theta_r - za*sr),
                 upper = c(theta_o + za*so, 
                           mu_p + za*sqrt(s2_p), 
                           0 + za*tau,
                           theta_r + za*sr),
                 p = c(signif(2*pnorm(q = zo, lower.tail = FALSE), 1), 
                       NA, 
                       NA, 
                       signif(2*pnorm(q = zr, lower.tail = FALSE), 2)))

plot.default(x = df$estimate, y = df$theta, type = "p", ylim = c(-0.3, 1), 
             xlim = c(0.4, 4.5), xaxt = "n", pch = 20, xlab = "", cex = 1.5,
             ylab = "Effect size", las = 1)
axis(side = 1, at = df$estimate, labels = levels(df$estimate), cex.axis = 0.8)
abline(h = 0, lty = 2)
arrows(x0 = as.numeric(df$estimate), y0 = df$lower, y1 = df$upper,
       length = 0.05, angle = 90, code = 3)
text(x = 4 + 0.3, y = 0.9, labels = bquote(italic(p)[S] == .(ps_tilde)), col = 2)
text(x = 0.6, y = theta_o + 0.05, labels = bquote(hat(theta)[o] == .(theta_o)))
text(x = 3.65, y = theta_r + 0.05, labels = bquote(hat(theta)[r] == .(theta_r)))
text(x = 0.65, y = theta_o - 0.1, labels = bquote(italic(p)[o] == .(df$p[1])))
text(x = 3.7, y = theta_r - 0.1, labels = bquote(italic(p)[r] == .(df$p[4])))
points(x = 2, y = 0, col = 2)
arrows(x0 = 1.8, x1 = 1.97, y0 = -0.2, y1 = -0.03, col = 2, length = 0.1)
text(x = 1.7, y = -0.25, labels = "fixed at zero", col = 2, cex = 0.8)
@
\caption{Example of assessment of replication success with one-sided sceptical $p$-value $p_S$.}
\end{figure}

The smallest level $\alpha$ at which replication success can be declared corresponds
to the sceptical $p$-value, analogous to the duality of ordinary $p$-values and 
confidence intervals (for technical details, see the \href{https://doi.org/10.1111/rssa.12493}{article}).


This method provides a theoretically sound approach to quantify 
replication success and it has attractive properties. In particular, the sceptical $p$-value is never smaller than 
the ordinary $p$-values from both studies and it also takes into account the size of 
the effect estimates, \ie it becomes larger if the replication estimate is smaller than
the original estimate. 
\citet{Held2019b} further expanded on the calibration of the sceptical $p$-value. The function \texttt{thresholdSceptical} allows to compute different types of thresholds for the sceptical
$p$-value with different type I error control properties.

<< "threshold-p-sceptical" >>=
## computing nominal, controlled, and liberal thresholds for one-sided sceptical p-value
(thresh_nom <- thresholdSceptical(level = 0.025, alternative = "one.sided", 
                                  type = "nominal"))
(thresh_contr <- thresholdSceptical(level = 0.025, alternative = "one.sided", 
                                    type = "controlled"))
(thresh_lib <- thresholdSceptical(level = 0.025, alternative = "one.sided", 
                                  type = "liberal"))
@

In particular, $p_S$ = \Sexpr{round(thresh_contr,3)} is the threshold for one-sided type I error control at $0.025^2 = 0.000625$ if replication and original estimate have equal variances. 

The sceptical $p$-value can be easily computed with the function \texttt{pSceptical}.
For the analysis of replication studies, it is recommended to report the one-sided sceptical $p$-value. 

<< "plot-pSceptical-projects", fig.height = 4 >>=
## computing one.sided sceptical p-value for replication projects
RProjects$ps <- with(RProjects, 
                     pSceptical(zo = zo, zr = zr, c = c, 
                                alternative = "one.sided"))
boxplot(ps ~ project, data = RProjects, las = 1, cex.axis = 0.7, ylim = c(0, 1),
        xlab = "Project", ylab = expression(italic(p)[S]), outline = FALSE)
abline(h = thresh_contr, lty = 3)
stripchart(ps ~ project, data = RProjects, vertical = TRUE, add = TRUE,
           pch = 19, method = "jitter", jitter = 0.2, cex = 0.6)

for (p in unique(RProjects$project)) {
    data_project <- subset(RProjects, project == p)
    cat(paste0(p, ": \n"))
    success_scept <- (data_project$ps < thresh_contr)
    cat(paste0(round(mean(success_scept)*100, 2), 
               "% smaller than 0.0653 (one-sided sceptical p-value) \n"))
    success_tradit <- (data_project$po1 < 0.025) & (data_project$pr1 < 0.025)
    cat(paste0(round(mean(success_tradit)*100, 2), 
               "% smaller than 0.025 (both one-sided traditional p-values) \n"))
    if(sum(success_scept != success_tradit) > 0){
        discrep <- data_project[(success_scept != success_tradit), 
                                c("ro", "rr", "c", "po1", "pr1", "ps")]
        ## print effect estimates, 1sided p-values, and c of discrepant studies
        cat("Discrepant studies: \n")
        print(signif(discrep, 2), row.names = FALSE)
  }
  cat("\n \n")
}
@ 

Thresholding the one-sided traditional $p$-values and the one-sided
sceptical $p$-values at the appropriate thresholds for type I error
control, we can see some discrepencies between the two. In particular,
the sceptical $p$-value may not indicate replication success 
when there is substantial shrinkage of the replication 
effect estimate relative to the original one, even if both estimates are significant. 



\paragraph{Design}
Design works similarly as for the statistical significance analysis strategy;
Using the function \texttt{sampleSizeReplicationSuccess}, one needs to choose a design 
prior, a sceptical $p$-value level, and the desired power to obain the required
relative sample size $c = n_r/n_o$.
The following code shows a few examples. 

<< >>=
sampleSizeReplicationSuccess(zo = 2.5, power = 0.8, level = thresh_contr,
                             alternative = "one.sided",
                             designPrior = "conditional")
sampleSizeReplicationSuccess(zo = 2.5, power = 0.8, level = thresh_contr,
                             alternative = "one.sided",
                             designPrior = "predictive")
@

Figure \ref{fig:powerReplicationSuccess} shows the power to achieve a one-sided 
sceptical $p$-value smaller or equal 0.0653 as a function of the $p$-value or $z$-value
of original study, assuming equal sample sizes in original and replication studies. The probability for replication success if the original study showed only weak evidence ($p_o = 0.05$) is now smaller than 0.5, which is reached for an original $p$-value of slightly above 0.03.
\begin{figure}[!h]
<< "plot-powerReplicationSuccess", echo = FALSE, fig.height = 4 >>=
## plot power
plot(po, powerReplicationSuccess(zo = p2z(po), 
                                 designPrior = "conditional",
                                 level = thresh_contr, 
                                 alternative = "one.sided")*100,
     type = "l", ylim = c(0, 100), lwd = 1.5, ylab = "Power (%)", las = 1,
     xlab = expression(italic(p)[o]))
axis(side = 3, at = seq(0.0, 0.05, by = 0.01), 
     labels = c("", round(p2z(p = seq(0.01, 0.05, by = 0.01)), 2)))
mtext(text = expression(paste("|", italic(z)[o], "|")), side = 3, line = 2)
lines(po, powerReplicationSuccess(zo = p2z(po), 
                                  designPrior = "predictive",
                                  level = thresh_contr, 
                                  alternative = "one.sided")*100, 
      lwd = 2, lty = 2)
legend("topright", legend = c("conditional", "predictive"), 
       title = "Design prior", lty = c(1, 2), lwd = 1.5, bty = "n")
abline(h = 50, lty = 3)
@
\caption{Power to achieve replication success 
(at the one-sided 0.0653 level) as a function of the two-sided $p$-value or $z$-value of original study.}
\label{fig:powerReplicationSuccess}
\end{figure}

Figure \ref{fig:sampleSizeReplicationSuccess} shows the required sample size to achieve
a one-sided sceptical $p$-value of $0.0653$ with 80\% power. The relative sample sizes consequently increase with increasing original $p$-value, with a dramatic increase for $p$-value larger than $0.023$ when the predictive design prior is used.
\begin{figure}[!h]
<< "plot-sampleSizeReplicationSuccess", echo = FALSE, fig.height = 4 >>=
po <- seq(0.0001, 0.05, 0.0001)

## plot sample size 
plot(po, 
     sampleSizeReplicationSuccess(zo = p2z(po), 
                                  designPrior = "conditional",
                                  level = thresh_contr, 
                                  alternative = "one.sided", 
                                  power = 0.8),
     type = "l", log = "y", lwd = 1.5, las = 1, ylim = c(0.5, 50),
     ylab = expression(paste("Relative sample size ", n[r]/n[o])),
     xlab = expression(italic(p)[o]), yaxt = "n")
axis(side = 2, las = 1, at = c(0.5, 1, 2, 5, 10, 20, 50), 
     labels = c("1/2", "1", "2", "5", "10", "20", "50"))
abline(h = 1, lty = 3)
axis(side = 3, at = seq(0.0, 0.05, by = 0.01), 
     labels = c("", round(p2z(p = seq(0.01, 0.05, by = 0.01)), 2)))
mtext(text = expression(paste("|", italic(z)[o], "|")), side = 3, line = 2)
lines(po, sampleSizeReplicationSuccess(zo = p2z(po),
                                       designPrior = "predictive", 
                                       level = thresh_contr, 
                                       alternative = "one.sided", 
                                       power = 0.8), 
      lwd = 2, lty = 2)
legend("topleft", legend = c("conditional", "predictive"), 
       title = "Design prior", lty = c(1, 2), lwd = 1.5, bty = "n")
@
\caption{Relative sample size to achieve replication success 
  (at the one-sided 0.0653 level) with 80\% power as
  a function of (two-sided) $p$-value or $z$-value of original study.}
\label{fig:sampleSizeReplicationSuccess}
\end{figure}


\section{Special topics}

\subsection{Interim analysis}
% + Add the reference of paper to come
Adaptive designs are a type of designs where one or more interim analyses are planned during the course of a study. This topic has extensively been studied and used in clinical trials for example, where continuing a study that should be stopped may lead to serious consequences. However, this type of design has not be covered in the framework of replication studies. \texttt{ReplicationSuccess} allows to calculate the power of the replication study after an interim analysis has been performed, taking into account the results from the first part of the study. The function \texttt{powerSignificanceInterim} is an extension of \texttt{powerSignificance} and requires in addition the specification of \texttt{zi}, the $z$-value at the interim analysis and \texttt{f}, the fraction of the replication study already completed. Moreover, the argument \texttt{designPrior} can be set to  \texttt{conditional}, \texttt{informed predictive} and \texttt{predictive}. Finally, the argument \texttt{analysisPrior} allows to also take the original result into account in the analysis of the replication study. 

Figure~\ref{fig:PowerSignificanceInterim} shows the conditional and the predictive power of the replication studies that continued into stage 2. While the condition power is larger than 80\% for all the studies, the predictive power is close to 0\% for some studies and always smaller than the conditional power.

\begin{figure}
<<echo = FALSE>>=
data("SSRP")
par(las = 1)
intpow_cond <- with(SSRP, powerSignificanceInterim(zo = fiso/se_fiso, zi = fisi/se_fisi, c = nr/no, 
                                                   f = ni/nr, designPrior = "conditional"))
intpow_pred <- with(SSRP, powerSignificanceInterim(zo = fiso/se_fiso, zi = fisi/se_fisi, c = nr/no, 
                                                   f = ni/nr, designPrior = "predictive"))
plot(intpow_cond*100, intpow_pred*100,  
     xlab = "Conditional power (in %)", 
     ylab = "Predictive power (in %)", 
     pch = 20,
     cex = 1.5,
     xlim = c(80,100), 
     ylim = c(0,100))
abline(a = 0, b = 1, col = "grey")
@
\caption{Conditional vs.\ predictive power at interim of the 10 studies from the social science replication project that were not stopped after stage 1. The grey line indicates the same value for conditional and predictive power.}
\label{fig:PowerSignificanceInterim}
\end{figure}

\subsection{Between-study heterogeneity}
It is  likely that the effect estimates from original and replication studies 
are not realizations of the exact same underlying effect size, but that there is 
between-study heterogeneity of effects. This can be caused, for example, if the replication
study is conducted in a different laboratory with different equipment. For this reason, 
many functions in \texttt{ReplicationSuccess} allow to incorporate additionally
uncertainty due to between-study heterogeneity into the predictive model. For example,
\texttt{sampleSizeSignificance} or \texttt{predictionInterval} allow to specify 
\texttt{d}, the relative between-study heterogeneity variance $d = \tau^2/\sigma^2$, 
\ie the ratio of the heterogeneity variance to the variance of the original effect estimate.
By default, \texttt{d} is set to zero, however, if between-study heterogeneity is
expected, \eg a different population of study participants is used, this should
be considered in the design. For details, see \citet{Pawel2020}.

\subsection{Data-driven shrinkage with empirical Bayes}
As previously mentioned, the functions \texttt{sampleSizeSignificance} and 
\texttt{powerSignificance} allow to specify the argument \texttt{shrinkage}, in order
to shrink the original effect estimate towards zero by a certain (arbitrary) amount. 
A more principled approach is to use a design prior which induces
shrinkage and then estimate the prior variance by empirical Bayes. This leads to
``data-driven'' shrinkage that is larger when there was only weak evidence for the
effect, and smaller when there was strong evidence for the effect (shown in Figure
\ref{fig:ebshrinkage}). 
Furthermore, under this prior, the specified between-study heterogeneity will also 
induce shrinkage towards zero, for details see \citet{Pawel2020}.
Empirical Bayes shrinkage is currently supported for the functions 
\texttt{sampleSizeSignificance}, \texttt{powerSignificance}, and 
\texttt{predictionInterval} by setting the design prior argument to \texttt{"EB"}.

\begin{figure}
<< "shrinkage", echo = FALSE, fig.height = 3.5 >>=
formatpval <- function (x, break.eps = 1e-04, break.middle = 0.01, na.form = "NA", 
    ...) 
{
    format1Pval <- function(pv) {
        if (is.na(pv)) {
            na.form
        }
        else if (pv < break.eps) {
            paste("<", format(break.eps, scientific = FALSE))
        }
        else {
            largep <- pv >= break.middle
            format(pv, digits = 1 + largep, nsmall = 1 + largep, 
                scientific = FALSE, ...)
        }
    }
    vapply(X = x, FUN = format1Pval, FUN.VALUE = "", USE.NAMES = TRUE)
}

zo <- seq(0, 4, 0.01)
s <- pmax(1 - 1/zo^2, 0)
plot(zo, s, type = "l", ylim = c(0, 1), las = 1,
     xlab = expression(paste("|",italic(z)[o],"|")), ylab = "Shrinkage factor")
axis(side = 3, at = seq(0, 4, by = 1), labels = formatpval(z2p(seq(0, 4, by = 1))))
mtext(text = expression(italic(p)[o]), side = 3, line = 2)
@
\caption{Empirical Bayes shrinkage when there is no between-study heterogeneity.}
\label{fig:ebshrinkage}
\end{figure}

\bibliography{bibliography}

\end{document}
